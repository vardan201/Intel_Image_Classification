# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zqd_vHZYPk_BuMEWEcvhe1MQu8eVdciW
"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d puneet6060/intel-image-classification

import zipfile
zip_ref = zipfile.ZipFile('/content/intel-image-classification.zip', 'r')
zip_ref.extractall('/content')
zip_ref.close()

#DATA AUGMENTATION
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.resnet50 import preprocess_input

IMG_SIZE = 224
BATCH_SIZE = 32
AUTOTUNE = tf.data.AUTOTUNE
NUM_CLASSES = 6  # for Intel dataset

# Set path to your dataset
TRAIN_DIR = "/content/seg_train/seg_train"
VAL_DIR = "/content/seg_test/seg_test"

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    TRAIN_DIR,
    image_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    label_mode="int",  # for sparse_categorical_crossentropy
    shuffle=True
)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    VAL_DIR,
    image_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    label_mode="int",
    shuffle=False
)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow.keras.applications import ResNet50 # Import ResNet50
from tensorflow.keras import models, layers, optimizers, callbacks, regularizers # Import necessary modules
from tensorflow.keras.backend import clear_session # Import clear_session

def full_augmentation(images, labels):
    # Apply augmentations to each image in the batch
    def apply_augmentation_to_single_image(image):
        image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])

        # Normal augmentations
        image = tf.image.random_flip_left_right(image)
        image = tf.image.random_brightness(image, max_delta=0.2)
        image = tf.image.random_contrast(image, lower=0.8, upper=1.2)
        image = tf.image.random_saturation(image, lower=0.8, upper=1.2)
        image = tf.image.random_hue(image, max_delta=0.02)

        # Zoom-in (simulate with random crop + resize)
        if tf.random.uniform(()) > 0.5:
            # tf.image.random_crop expects a single image, not a batch
            crop_size = tf.random.uniform([], int(0.8 * IMG_SIZE), IMG_SIZE, dtype=tf.int32)
            image = tf.image.random_crop(image, size=[crop_size, crop_size, 3])
            image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])

        # Advanced augmentation: Cutout
        if tf.random.uniform(()) > 0.5:
            cutout_size = 50
            y = tf.random.uniform([], 0, IMG_SIZE - cutout_size, dtype=tf.int32)
            x = tf.random.uniform([], 0, IMG_SIZE - cutout_size, dtype=tf.int32)

            # Generate indices tensorically instead of using Python list comprehension
            y_indices = tf.range(y, y + cutout_size)
            x_indices = tf.range(x, x + cutout_size)
            channel_indices = tf.range(3)

            Y, X, C = tf.meshgrid(y_indices, x_indices, channel_indices, indexing='ij')
            indices = tf.stack([Y, X, C], axis=-1)
            indices = tf.reshape(indices, [-1, 3])

            mask = tf.ones((IMG_SIZE, IMG_SIZE, 3))
            # Create updates tensor with zeros
            updates = tf.zeros_like(indices, dtype=tf.float32)[:, 0]
            updates = tf.reshape(updates, [-1, 1]) # Ensure correct shape for scatter_nd_update
            updates = tf.tile(updates, [1, 3]) # Tile for 3 channels
            updates = tf.reshape(updates, [-1]) # Flatten to match scatter_nd_update updates shape

            # The updates need to be a flat list of values for each scattered index
            # Since we are setting to 0.0 for all channels, we can just repeat 0.0
            updates_flat = tf.zeros(tf.shape(indices)[0], dtype=tf.float32)

            mask = tf.tensor_scatter_nd_update(
                mask,
                indices,
                updates_flat
            )
            image = image * mask

        # Final preprocessing for ResNet
        image = preprocess_input(image)
        return image

    # Use tf.map_fn to apply the augmentation function to each image in the batch
    # The fn_output_signature should match the output of apply_augmentation_to_single_image
    # which is a single image tensor. The labels are passed through separately.
    augmented_images = tf.map_fn(apply_augmentation_to_single_image, images, fn_output_signature=tf.float32)

    return augmented_images, labels

# Validation preprocessing
def preprocess_val(image, label):
    # No need for tf.map_fn here as random_crop is not used
    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])
    image = preprocess_input(image)
    return image, label

train_ds = train_ds.map(full_augmentation, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)
val_ds = val_ds.map(preprocess_val, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)

def objective(trial):
    clear_session()

    # Hyperparameters
    dense_units = trial.suggest_categorical('dense_units', [128, 256, 512])
    dropout_rate = trial.suggest_float('dropout_rate', 0.3, 0.6)
    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)
    unfreeze_layers = trial.suggest_int('unfreeze_layers', 10, 50, step=10)
    l2_reg = trial.suggest_float('l2_reg', 1e-5, 1e-3, log=True)

    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))
    base_model.trainable = True

    # Freeze layers except last `unfreeze_layers`
    for layer in base_model.layers[:-unfreeze_layers]:
        layer.trainable = False

    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dense(dense_units, kernel_regularizer=regularizers.l2(l2_reg)),
        layers.BatchNormalization(),
        layers.Activation('relu'),
        layers.Dropout(dropout_rate),
        layers.Dense(NUM_CLASSES, activation='softmax', kernel_regularizer=regularizers.l2(l2_reg))  # multi-class output
    ])

    model.compile(
        optimizer=optimizers.Adam(learning_rate=learning_rate),
        loss='sparse_categorical_crossentropy',  # correct for integer labels
        metrics=['accuracy']
    )

    es = callbacks.EarlyStopping(monitor='val_accuracy', patience=2, restore_best_weights=True)

    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=10,
        callbacks=[es],
        verbose=1
    )

    val_acc = max(history.history['val_accuracy'])
    return val_acc

!pip install optuna
import optuna

study = optuna.create_study(direction='maximize')  # maximize val_accuracy
study.optimize(objective, n_trials=2)  # run 10 trials (you can increase)

from tensorflow.keras.applications import ResNet50
from tensorflow.keras import layers, models, optimizers, regularizers
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.backend import clear_session

# Clear previous session
clear_session()

# Use the best hyperparameters from Optuna
dense_units = 256
dropout_rate = 0.34122082765853473
learning_rate = 0.00025536314630144106
unfreeze_layers = 10
l2_reg = 0.00023034292982793486

# Load ResNet50 base model
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))
base_model.trainable = True

# Freeze all layers except the last `unfreeze_layers`
for layer in base_model.layers[:-unfreeze_layers]:
    layer.trainable = False

# Build the model
model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(dense_units, kernel_regularizer=regularizers.l2(l2_reg)),
    layers.BatchNormalization(),
    layers.Activation('relu'),
    layers.Dropout(dropout_rate),
    layers.Dense(NUM_CLASSES, activation='softmax', kernel_regularizer=regularizers.l2(l2_reg))  # use softmax for multi-class
])

# Compile the model
model.compile(
    optimizer=optimizers.Adam(learning_rate=learning_rate),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Add callbacks
early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, restore_best_weights=True)

# Train the model
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,
    callbacks=[early_stopping],
    verbose=1
)

model.save("scenery_model.keras")

model.save("/content/scenery_model.keras")  # For Colab

!ls -lh /content/*.keras
# or
!ls -lh /content/*.h5

from google.colab import files
files.download('/content/scenery_model.keras')

